# scraper.py

import json
import time
import configparser
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import Select
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- C√ÅC H·∫∞NG S·ªê V√Ä URL ---

BASE_URL = "https://fap.fpt.edu.vn/"
LOGIN_URL = BASE_URL + "Default.aspx"
HOME_PAGE_URL = BASE_URL + "Student.aspx"
SCHEDULE_URL = BASE_URL + "Report/ScheduleOfWeek.aspx"
EXAM_SCHEDULE_URL = BASE_URL + "Exam/ScheduleExams.aspx"
MARK_REPORT_URL = BASE_URL + "Grade/StudentGrade.aspx"
ATTENDANCE_URL = BASE_URL + "Report/ViewAttendstudent.aspx"


# --- C√ÅC H√ÄM TI·ªÜN √çCH ---

def save_to_json(data, filename):
    """L∆∞u d·ªØ li·ªáu v√†o file JSON v·ªõi ƒë·ªãnh d·∫°ng ƒë·∫πp."""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu th√†nh c√¥ng v√†o file: {filename}")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u file {filename}: {e}")

def get_config():
    """ƒê·ªçc th√¥ng tin c·∫•u h√¨nh t·ª´ file config.ini."""
    config = configparser.ConfigParser()
    config.read('config.ini')
    return config['FAP_CREDENTIALS']


# --- C√ÅC H√ÄM C√ÄO D·ªÆ LI·ªÜU ---

def login(driver, username, password, campus_name):
    """Th·ª±c hi·ªán qu√° tr√¨nh ƒëƒÉng nh·∫≠p v√†o FAP."""
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh ƒëƒÉng nh·∫≠p...")
    try:
        driver.get(LOGIN_URL)
        time.sleep(2)

        # Ch·ªçn campus
        campus_dropdown = Select(driver.find_element(By.ID, "ctl00_mainContent_ddlCampus"))
        campus_dropdown.select_by_visible_text(campus_name)
        time.sleep(1)

        # Nh·∫•n n√∫t Login with FeID
        driver.find_element(By.ID, "ctl00_mainContent_btnloginFeId").click()
        time.sleep(4) # Ch·ªù trang SSO chuy·ªÉn h∆∞·ªõng

        # ƒêi·ªÅn th√¥ng tin ƒëƒÉng nh·∫≠p tr√™n trang SSO
        print("üîë ƒêang ·ªü trang SSO, ti·∫øn h√†nh nh·∫≠p th√¥ng tin...")
        driver.find_element(By.ID, "Username").send_keys(username)
        driver.find_element(By.ID, "Password").send_keys(password)
        driver.find_element(By.CSS_SELECTOR, "button[type='submit']").click()
        time.sleep(5) # Ch·ªù ƒëƒÉng nh·∫≠p v√† chuy·ªÉn h∆∞·ªõng v·ªÅ trang ch·ªß

        # Ki·ªÉm tra ƒëƒÉng nh·∫≠p th√†nh c√¥ng
        if "Student.aspx" in driver.current_url:
            print("‚úÖ ƒêƒÉng nh·∫≠p th√†nh c√¥ng!")
            return True
        else:
            print("‚ùå ƒêƒÉng nh·∫≠p th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i t√†i kho·∫£n ho·∫∑c m·∫≠t kh·∫©u.")
            return False
    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh ƒëƒÉng nh·∫≠p: {e}")
        return False

def scrape_profile(driver):
    """C√†o th√¥ng tin c∆° b·∫£n c·ªßa sinh vi√™n."""
    print("üë§ ƒêang c√†o th√¥ng tin c√° nh√¢n (profile)...")
    driver.get(HOME_PAGE_URL)
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    profile_data = {
        "lastUpdated": datetime.now().isoformat()
    }
    try:
        # L·∫•y email v√† campus t·ª´ header
        email_span = soup.find("span", {"id": "ctl00_lblLogIn"})
        campus_span = soup.find("span", {"id": "ctl00_lblCampusName"})
        
        # Tr√≠ch xu·∫•t v√† l√†m s·∫°ch d·ªØ li·ªáu
        email_full = email_span.get_text(strip=True) if email_span else "Kh√¥ng t√¨m th·∫•y"
        # T√°ch t√™n v√† m√£ SV t·ª´ email
        profile_data['email'] = email_full
        if '(' in email_full and ')' in email_full:
            parts = email_full.split('(')
            profile_data['fullName'] = parts[0].strip()
            profile_data['studentId'] = parts[1].replace(')', '').strip()
        else: # Fallback n·∫øu kh√¥ng c√≥ ƒë·ªãnh d·∫°ng mong mu·ªën
             profile_data['fullName'] = "Kh√¥ng t√¨m th·∫•y"
             profile_data['studentId'] = "Kh√¥ng t√¨m th·∫•y"

        profile_data['campus'] = campus_span.get_text(strip=True).replace('CAMPUS: ', '') if campus_span else "Kh√¥ng t√¨m th·∫•y"

    except Exception as e:
        print(f"‚ùå L·ªói khi c√†o th√¥ng tin profile: {e}")

    return profile_data


def scrape_exam_schedule(driver):
    """C√†o d·ªØ li·ªáu l·ªãch thi."""
    print("üìÖ ƒêang c√†o l·ªãch thi...")
    driver.get(EXAM_SCHEDULE_URL)
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    exam_data = {
        "lastUpdated": datetime.now().isoformat(),
        "exams": []
    }
    
    try:
        table = soup.find("div", {"id": "ctl00_mainContent_divContent"}).find("table")
        if not table:
            return exam_data

        rows = table.find_all("tr")[1:] # B·ªè qua h√†ng header
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 9:
                exam = {
                    "subjectCode": cols[1].get_text(strip=True),
                    "subjectName": cols[2].get_text(strip=True),
                    "date": cols[3].get_text(strip=True),
                    "room": cols[4].get_text(strip=True) or None,
                    "time": cols[5].get_text(strip=True),
                    "type": cols[7].get_text(strip=True),
                    "format": cols[6].get_text(strip=True),
                    "publicationDate": cols[8].get_text(strip=True)
                }
                exam_data["exams"].append(exam)
    except Exception as e:
        print(f"‚ùå L·ªói khi c√†o l·ªãch thi: {e}")

    return exam_data

def scrape_grades_and_attendance(driver):
    """C√†o ƒëi·ªÉm v√† ƒëi·ªÉm danh c·ªßa t·∫•t c·∫£ c√°c k·ª≥."""
    print("üìä ƒêang c√†o ƒëi·ªÉm v√† ƒëi·ªÉm danh...")
    
    # --- C√†o ƒëi·ªÉm ---
    driver.get(MARK_REPORT_URL)
    time.sleep(2)
    grades_soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    grades_data = {"semesters": []}
    
    try:
        term_links = grades_soup.select("#ctl00_mainContent_divTerm a")
        for term_link in term_links:
            term_name = term_link.get_text(strip=True)
            term_url = BASE_URL + "Grade/" + term_link['href']
            print(f"  -> ƒêang x·ª≠ l√Ω k·ª≥: {term_name}")
            
            semester_obj = {"term": term_name, "courses": []}
            
            driver.get(term_url)
            time.sleep(2)
            term_soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            course_links = term_soup.select("#ctl00_mainContent_divCourse a")
            for course_link in course_links:
                course_name_full = course_link.get_text(strip=True)
                course_url = BASE_URL + "Grade/" + course_link['href']
                
                # Tr√≠ch xu·∫•t m√£ m√¥n
                subject_code = course_name_full.split('(')[1].split(')')[0]
                
                driver.get(course_url)
                time.sleep(2)
                course_soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                course_obj = {
                    "subjectCode": subject_code,
                    "subjectName": course_name_full.split('(')[0].strip(),
                    "gradeDetails": []
                }
                
                grade_table = course_soup.select_one("#ctl00_mainContent_divGrade table")
                if grade_table:
                    rows = grade_table.find_all("tr")[1:] # B·ªè header
                    current_category = ""
                    for row in rows:
                        cols = row.find_all("td")
                        # X√°c ƒë·ªãnh category
                        if cols[0].has_attr('rowspan'):
                            current_category = cols[0].get_text(strip=True)
                            item_col_index = 1
                        else:
                            item_col_index = 0

                        # X·ª≠ l√Ω c√°c d√≤ng t·ªïng k·∫øt v√† tr·∫°ng th√°i
                        if "Course total" in current_category:
                            if "Average" in cols[item_col_index].get_text(strip=True):
                                course_obj["average"] = float(cols[item_col_index + 2].get_text(strip=True))
                            elif "Status" in cols[item_col_index].get_text(strip=True):
                                course_obj["status"] = cols[item_col_index + 2].get_text(strip=True)
                            continue

                        # X·ª≠ l√Ω c√°c d√≤ng ƒëi·ªÉm th√†nh ph·∫ßn
                        if len(cols) > (item_col_index + 2):
                             # Chuy·ªÉn ƒë·ªïi weight v√† value th√†nh s·ªë, x·ª≠ l√Ω l·ªói
                            try:
                                weight_val = float(cols[item_col_index + 1].get_text(strip=True).replace('%', ''))
                            except (ValueError, IndexError):
                                weight_val = None
                            
                            try:
                                value_val = float(cols[item_col_index + 2].get_text(strip=True))
                            except (ValueError, IndexError):
                                value_val = None

                            grade_item = {
                                "category": current_category,
                                "item": cols[item_col_index].get_text(strip=True),
                                "weight": weight_val,
                                "value": value_val
                            }
                            course_obj["gradeDetails"].append(grade_item)

                semester_obj["courses"].append(course_obj)
            grades_data["semesters"].append(semester_obj)

    except Exception as e:
        print(f"‚ùå L·ªói khi c√†o ƒëi·ªÉm: {e}")
    
    # --- C√†o ƒëi·ªÉm danh ---
    # (B·∫°n c√≥ th·ªÉ t√≠ch h·ª£p logic c√†o ƒëi·ªÉm danh t∆∞∆°ng t·ª± v√†o v√≤ng l·∫∑p ·ªü tr√™n
    # ƒë·ªÉ tr√°nh l·∫∑p l·∫°i vi·ªác duy·ªát qua c√°c k·ª≥ v√† m√¥n h·ªçc)
    # V√¨ m·ª•c ƒë√≠ch minh h·ªça, ph·∫ßn n√†y ƒë∆∞·ª£c t√°ch ri√™ng
    print("üìã Hi·ªán t·∫°i, script n√†y t·∫≠p trung c√†o ƒëi·ªÉm. Ph·∫ßn ƒëi·ªÉm danh c√≥ th·ªÉ ƒë∆∞·ª£c th√™m v√†o sau.")

    return grades_data

# --- H√ÄM CH√çNH ---

def main():
    """H√†m ch√≠nh ƒëi·ªÅu khi·ªÉn to√†n b·ªô qu√° tr√¨nh."""
    config = get_config()
    username = config.get('USERNAME')
    password = config.get('PASSWORD')
    campus_name = config.get('CAMPUS')

    if "your_feid_username_here" in username:
        print("‚ùå Vui l√≤ng c·∫≠p nh·∫≠t th√¥ng tin ƒëƒÉng nh·∫≠p trong file `config.ini` tr∆∞·ªõc khi ch·∫°y.")
        return

    # Kh·ªüi t·∫°o Selenium WebDriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)
    driver.implicitly_wait(10) # Ch·ªù t·ªëi ƒëa 10s ƒë·ªÉ t√¨m element

    if login(driver, username, password, campus_name):
        # T·∫°o m·ªôt dictionary ƒë·ªÉ l∆∞u t·∫•t c·∫£ d·ªØ li·ªáu
        all_data = {}
        
        # C√†o d·ªØ li·ªáu
        all_data['profile'] = scrape_profile(driver)
        all_data['exam_schedule'] = scrape_exam_schedule(driver)
        # H√†m scrape_grades_and_attendance ƒë√£ ph·ª©c t·∫°p, s·∫Ω ch·∫°y ri√™ng
        grades_data = scrape_grades_and_attendance(driver)

        # L∆∞u file JSON
        save_to_json(all_data['profile'], 'profile.json')
        save_to_json(all_data['exam_schedule'], 'exam_schedule.json')
        save_to_json(grades_data, 'grades.json')
        
        # C√°c h√†m c√†o d·ªØ li·ªáu kh√°c c√≥ th·ªÉ ƒë∆∞·ª£c g·ªçi ·ªü ƒë√¢y
        # save_to_json(scrape_schedule(driver), 'schedule.json')
        # ...
        
        print("\nüéâ Ho√†n th√†nh! T·∫•t c·∫£ c√°c file JSON ƒë√£ ƒë∆∞·ª£c t·∫°o.")
    else:
        print("Kh√¥ng th·ªÉ ti·∫øp t·ª•c v√¨ ƒëƒÉng nh·∫≠p kh√¥ng th√†nh c√¥ng.")
        
    driver.quit()

if __name__ == "__main__":
    main()