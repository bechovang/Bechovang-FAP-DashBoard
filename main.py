# scraper.py

import json
import time
import configparser
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import Select
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- CÃC Háº°NG Sá» VÃ€ URL ---

BASE_URL = "https://fap.fpt.edu.vn/"
LOGIN_URL = BASE_URL + "Default.aspx"
HOME_PAGE_URL = BASE_URL + "Student.aspx"
SCHEDULE_URL = BASE_URL + "Report/ScheduleOfWeek.aspx"
EXAM_SCHEDULE_URL = BASE_URL + "Exam/ScheduleExams.aspx"
MARK_REPORT_URL = BASE_URL + "Grade/StudentGrade.aspx"
ATTENDANCE_URL = BASE_URL + "Report/ViewAttendstudent.aspx"


# --- CÃC HÃ€M TIá»†N ÃCH ---

def save_to_json(data, filename):
    """LÆ°u dá»¯ liá»‡u vÃ o file JSON vá»›i Ä‘á»‹nh dáº¡ng Ä‘áº¹p."""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… ÄÃ£ lÆ°u dá»¯ liá»‡u thÃ nh cÃ´ng vÃ o file: {filename}")
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u file {filename}: {e}")

def get_config():
    """Äá»c thÃ´ng tin cáº¥u hÃ¬nh tá»« file config.ini."""
    config = configparser.ConfigParser()
    config.read('config.ini')
    return config['FAP_CREDENTIALS']


# --- CÃC HÃ€M CÃ€O Dá»® LIá»†U ---

def login(driver, username, password, campus_name):
    """Thá»±c hiá»‡n quÃ¡ trÃ¬nh Ä‘Äƒng nháº­p vÃ o FAP."""
    print("ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh Ä‘Äƒng nháº­p...")
    try:
        driver.get(LOGIN_URL)
        time.sleep(2)

        # Chá»n campus
        campus_dropdown = Select(driver.find_element(By.ID, "ctl00_mainContent_ddlCampus"))
        campus_dropdown.select_by_visible_text(campus_name)
        time.sleep(1)

        # Nháº¥n nÃºt Login with FeID
        driver.find_element(By.ID, "ctl00_mainContent_btnloginFeId").click()
        time.sleep(4) # Chá» trang SSO chuyá»ƒn hÆ°á»›ng

        # Äiá»n thÃ´ng tin Ä‘Äƒng nháº­p trÃªn trang SSO
        print("ğŸ”‘ Äang á»Ÿ trang SSO, tiáº¿n hÃ nh nháº­p thÃ´ng tin...")
        driver.find_element(By.ID, "Username").send_keys(username)
        driver.find_element(By.ID, "Password").send_keys(password)
        driver.find_element(By.CSS_SELECTOR, "button[type='submit']").click()
        time.sleep(5) # Chá» Ä‘Äƒng nháº­p vÃ  chuyá»ƒn hÆ°á»›ng vá» trang chá»§

        # Kiá»ƒm tra Ä‘Äƒng nháº­p thÃ nh cÃ´ng
        if "Student.aspx" in driver.current_url:
            print("âœ… ÄÄƒng nháº­p thÃ nh cÃ´ng!")
            return True
        else:
            print("âŒ ÄÄƒng nháº­p tháº¥t báº¡i. Vui lÃ²ng kiá»ƒm tra láº¡i tÃ i khoáº£n hoáº·c máº­t kháº©u.")
            return False
    except Exception as e:
        print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh Ä‘Äƒng nháº­p: {e}")
        return False

def scrape_profile(driver):
    """CÃ o thÃ´ng tin cÆ¡ báº£n cá»§a sinh viÃªn."""
    print("ğŸ‘¤ Äang cÃ o thÃ´ng tin cÃ¡ nhÃ¢n (profile)...")
    driver.get(HOME_PAGE_URL)
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    profile_data = {
        "lastUpdated": datetime.now().isoformat()
    }
    try:
        # Láº¥y email vÃ  campus tá»« header
        email_span = soup.find("span", {"id": "ctl00_lblLogIn"})
        campus_span = soup.find("span", {"id": "ctl00_lblCampusName"})
        
        # TrÃ­ch xuáº¥t vÃ  lÃ m sáº¡ch dá»¯ liá»‡u
        email_full = email_span.get_text(strip=True) if email_span else "KhÃ´ng tÃ¬m tháº¥y"
        # TÃ¡ch tÃªn vÃ  mÃ£ SV tá»« email
        profile_data['email'] = email_full
        if '(' in email_full and ')' in email_full:
            parts = email_full.split('(')
            profile_data['fullName'] = parts[0].strip()
            profile_data['studentId'] = parts[1].replace(')', '').strip()
        else: # Fallback náº¿u khÃ´ng cÃ³ Ä‘á»‹nh dáº¡ng mong muá»‘n
             profile_data['fullName'] = "KhÃ´ng tÃ¬m tháº¥y"
             profile_data['studentId'] = "KhÃ´ng tÃ¬m tháº¥y"

        profile_data['campus'] = campus_span.get_text(strip=True).replace('CAMPUS: ', '') if campus_span else "KhÃ´ng tÃ¬m tháº¥y"

    except Exception as e:
        print(f"âŒ Lá»—i khi cÃ o thÃ´ng tin profile: {e}")

    return profile_data


def scrape_exam_schedule(driver):
    """CÃ o dá»¯ liá»‡u lá»‹ch thi."""
    print("ğŸ“… Äang cÃ o lá»‹ch thi...")
    driver.get(EXAM_SCHEDULE_URL)
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    exam_data = {
        "lastUpdated": datetime.now().isoformat(),
        "exams": []
    }
    
    try:
        table = soup.find("div", {"id": "ctl00_mainContent_divContent"}).find("table")
        if not table:
            return exam_data

        rows = table.find_all("tr")[1:] # Bá» qua hÃ ng header
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 9:
                exam = {
                    "subjectCode": cols[1].get_text(strip=True),
                    "subjectName": cols[2].get_text(strip=True),
                    "date": cols[3].get_text(strip=True),
                    "room": cols[4].get_text(strip=True) or None,
                    "time": cols[5].get_text(strip=True),
                    "type": cols[7].get_text(strip=True),
                    "format": cols[6].get_text(strip=True),
                    "publicationDate": cols[8].get_text(strip=True)
                }
                exam_data["exams"].append(exam)
    except Exception as e:
        print(f"âŒ Lá»—i khi cÃ o lá»‹ch thi: {e}")

    return exam_data

def scrape_grades_and_attendance(driver):
    """CÃ o Ä‘iá»ƒm vÃ  Ä‘iá»ƒm danh cá»§a táº¥t cáº£ cÃ¡c ká»³."""
    print("ğŸ“Š Äang cÃ o Ä‘iá»ƒm vÃ  Ä‘iá»ƒm danh...")
    
    # --- CÃ o Ä‘iá»ƒm ---
    driver.get(MARK_REPORT_URL)
    time.sleep(2)
    grades_soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    grades_data = {"semesters": []}
    
    try:
        term_links = grades_soup.select("#ctl00_mainContent_divTerm a")
        for term_link in term_links:
            term_name = term_link.get_text(strip=True)
            term_url = BASE_URL + "Grade/" + term_link['href']
            print(f"  -> Äang xá»­ lÃ½ ká»³: {term_name}")
            
            semester_obj = {"term": term_name, "courses": []}
            
            driver.get(term_url)
            time.sleep(2)
            term_soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            course_links = term_soup.select("#ctl00_mainContent_divCourse a")
            for course_link in course_links:
                course_name_full = course_link.get_text(strip=True)
                course_url = BASE_URL + "Grade/" + course_link['href']
                
                # TrÃ­ch xuáº¥t mÃ£ mÃ´n
                subject_code = course_name_full.split('(')[1].split(')')[0]
                
                driver.get(course_url)
                time.sleep(2)
                course_soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                course_obj = {
                    "subjectCode": subject_code,
                    "subjectName": course_name_full.split('(')[0].strip(),
                    "gradeDetails": []
                }
                
                grade_table = course_soup.select_one("#ctl00_mainContent_divGrade table")
                if grade_table:
                    rows = grade_table.find_all("tr")[1:] # Bá» header
                    current_category = ""
                    for row in rows:
                        cols = row.find_all("td")
                        # XÃ¡c Ä‘á»‹nh category
                        if cols[0].has_attr('rowspan'):
                            current_category = cols[0].get_text(strip=True)
                            item_col_index = 1
                        else:
                            item_col_index = 0

                        # Xá»­ lÃ½ cÃ¡c dÃ²ng tá»•ng káº¿t vÃ  tráº¡ng thÃ¡i
                        if "Course total" in current_category:
                            if "Average" in cols[item_col_index].get_text(strip=True):
                                course_obj["average"] = float(cols[item_col_index + 2].get_text(strip=True))
                            elif "Status" in cols[item_col_index].get_text(strip=True):
                                course_obj["status"] = cols[item_col_index + 2].get_text(strip=True)
                            continue

                        # Xá»­ lÃ½ cÃ¡c dÃ²ng Ä‘iá»ƒm thÃ nh pháº§n
                        if len(cols) > (item_col_index + 2):
                             # Chuyá»ƒn Ä‘á»•i weight vÃ  value thÃ nh sá»‘, xá»­ lÃ½ lá»—i
                            try:
                                weight_val = float(cols[item_col_index + 1].get_text(strip=True).replace('%', ''))
                            except (ValueError, IndexError):
                                weight_val = None
                            
                            try:
                                value_val = float(cols[item_col_index + 2].get_text(strip=True))
                            except (ValueError, IndexError):
                                value_val = None

                            grade_item = {
                                "category": current_category,
                                "item": cols[item_col_index].get_text(strip=True),
                                "weight": weight_val,
                                "value": value_val
                            }
                            course_obj["gradeDetails"].append(grade_item)

                semester_obj["courses"].append(course_obj)
            grades_data["semesters"].append(semester_obj)

    except Exception as e:
        print(f"âŒ Lá»—i khi cÃ o Ä‘iá»ƒm: {e}")
    
    # --- CÃ o Ä‘iá»ƒm danh ---
    # (Báº¡n cÃ³ thá»ƒ tÃ­ch há»£p logic cÃ o Ä‘iá»ƒm danh tÆ°Æ¡ng tá»± vÃ o vÃ²ng láº·p á»Ÿ trÃªn
    # Ä‘á»ƒ trÃ¡nh láº·p láº¡i viá»‡c duyá»‡t qua cÃ¡c ká»³ vÃ  mÃ´n há»c)
    # VÃ¬ má»¥c Ä‘Ã­ch minh há»a, pháº§n nÃ y Ä‘Æ°á»£c tÃ¡ch riÃªng
    print("ğŸ“‹ Hiá»‡n táº¡i, script nÃ y táº­p trung cÃ o Ä‘iá»ƒm. Pháº§n Ä‘iá»ƒm danh cÃ³ thá»ƒ Ä‘Æ°á»£c thÃªm vÃ o sau.")

    return grades_data

# --- HÃ€M CHÃNH ---

def main():
    """HÃ m chÃ­nh Ä‘iá»u khiá»ƒn toÃ n bá»™ quÃ¡ trÃ¬nh."""
    config = get_config()
    username = config.get('USERNAME')
    password = config.get('PASSWORD')
    campus_name = config.get('CAMPUS')

    if "your_feid_username_here" in username:
        print("âŒ Vui lÃ²ng cáº­p nháº­t thÃ´ng tin Ä‘Äƒng nháº­p trong file `config.ini` trÆ°á»›c khi cháº¡y.")
        return

    # Khá»Ÿi táº¡o Selenium WebDriver
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service)
    driver.implicitly_wait(10) # Chá» tá»‘i Ä‘a 10s Ä‘á»ƒ tÃ¬m element

    if login(driver, username, password, campus_name):
        # Táº¡o má»™t dictionary Ä‘á»ƒ lÆ°u táº¥t cáº£ dá»¯ liá»‡u
        all_data = {}
        
        # CÃ o dá»¯ liá»‡u
        all_data['profile'] = scrape_profile(driver)
        all_data['exam_schedule'] = scrape_exam_schedule(driver)
        # HÃ m scrape_grades_and_attendance Ä‘Ã£ phá»©c táº¡p, sáº½ cháº¡y riÃªng
        grades_data = scrape_grades_and_attendance(driver)

        # LÆ°u file JSON
        save_to_json(all_data['profile'], 'profile.json')
        save_to_json(all_data['exam_schedule'], 'exam_schedule.json')
        save_to_json(grades_data, 'grades.json')
        
        # CÃ¡c hÃ m cÃ o dá»¯ liá»‡u khÃ¡c cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i á»Ÿ Ä‘Ã¢y
        # save_to_json(scrape_schedule(driver), 'schedule.json')
        # ...
        
        print("\nğŸ‰ HoÃ n thÃ nh! Táº¥t cáº£ cÃ¡c file JSON Ä‘Ã£ Ä‘Æ°á»£c táº¡o.")
    else:
        print("KhÃ´ng thá»ƒ tiáº¿p tá»¥c vÃ¬ Ä‘Äƒng nháº­p khÃ´ng thÃ nh cÃ´ng.")
        
    driver.quit()

if __name__ == "__main__":
    main()